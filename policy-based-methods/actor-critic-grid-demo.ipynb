{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world environment\n",
    "GRID_ROWS, GRID_COLS = 4, 5  # 4x5 Grid World\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_space_size = len(actions)\n",
    "action_symbols = ['U', 'D', 'L', 'R']  # For display purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Table:\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the policy network parameters (random initialization for actor)\n",
    "theta = np.random.rand(GRID_ROWS, GRID_COLS, action_space_size)\n",
    "\n",
    "# Define the value function (critic)\n",
    "value_function = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "\n",
    "# Define learning rates\n",
    "alpha_theta = 0.1  # Policy (actor) learning rate\n",
    "alpha_v = 0.1      # Value (critic) learning rate\n",
    "gamma = 0.9        # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards grid\n",
    "rewards = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "rewards[3, 4] = 1  # Goal position\n",
    "\n",
    "# Function to print reward table\n",
    "def print_reward_table():\n",
    "    print(\"Reward Table:\")\n",
    "    print(rewards)\n",
    "    print(\"\\n\")\n",
    "print_reward_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy softmax function to get action probabilities\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# Function to choose action based on policy (probabilities from softmax)\n",
    "def choose_action(state):\n",
    "    row, col = state\n",
    "    action_probs = softmax(theta[row, col])\n",
    "    return np.random.choice(len(actions), p=action_probs)\n",
    "\n",
    "# Function to take a step in the grid world\n",
    "def take_step(state, action):\n",
    "    row, col = state\n",
    "\n",
    "    if action == 0:  # Up\n",
    "        new_state = (max(0, row - 1), col)\n",
    "    elif action == 1:  # Down\n",
    "        new_state = (min(GRID_ROWS - 1, row + 1), col)\n",
    "    elif action == 2:  # Left\n",
    "        new_state = (row, max(0, col - 1))\n",
    "    else:  # Right\n",
    "        new_state = (row, min(GRID_COLS - 1, col + 1))\n",
    "\n",
    "    reward = rewards[new_state]\n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Update\n",
    "def actor_critic_update(state, action, reward, next_state):\n",
    "    row, col = state\n",
    "    next_row, next_col = next_state\n",
    "    \n",
    "    # Critic Update (value function)\n",
    "    td_target = reward + gamma * value_function[next_row, next_col]\n",
    "    td_error = td_target - value_function[row, col]\n",
    "    value_function[row, col] += alpha_v * td_error  # Update value function\n",
    "    \n",
    "    # Actor Update (policy gradient)\n",
    "    action_probs = softmax(theta[row, col])\n",
    "    for a in range(action_space_size):\n",
    "        grad_log_pi = (1 if a == action else 0) - action_probs[a]  # ∇logπ for each action\n",
    "        theta[row, col, a] += alpha_theta * td_error * grad_log_pi  # Update policy\n",
    "\n",
    "# Function to display policy and probabilities side-by-side\n",
    "def display_policy():\n",
    "    direction_grid = np.full((GRID_ROWS, GRID_COLS), '', dtype='<U1')\n",
    "    probability_grid = np.zeros((GRID_ROWS, GRID_COLS))\n",
    "\n",
    "    for row in range(GRID_ROWS):\n",
    "        for col in range(GRID_COLS):\n",
    "            action_probs = softmax(theta[row, col])\n",
    "            max_action = np.argmax(action_probs)\n",
    "            direction_grid[row, col] = action_symbols[max_action]  # Direction with highest prob\n",
    "            probability_grid[row, col] = np.max(action_probs)      # Highest probability\n",
    "\n",
    "    # Display both grids side by side\n",
    "    print(\"Policy (Direction) | Probabilities (Max)\")\n",
    "    for row in range(GRID_ROWS):\n",
    "        direction_row = \" \".join(direction_grid[row])\n",
    "        probability_row = \" \".join(f\"{prob:.2f}\" for prob in probability_grid[row])\n",
    "        print(f\"{direction_row}      | {probability_row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.33 0.29 0.32 0.42 0.30\n",
      "U D L D R      | 0.38 0.37 0.36 0.34 0.33\n",
      "L D U R D      | 0.34 0.35 0.33 0.33 0.36\n",
      "U R L D L      | 0.29 0.37 0.31 0.30 0.39\n",
      "\n",
      "\n",
      "Iteration 20:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.33 0.29 0.32 0.42 0.30\n",
      "U D L D R      | 0.38 0.37 0.35 0.35 0.32\n",
      "L D U R D      | 0.34 0.36 0.33 0.32 0.39\n",
      "U R R R L      | 0.29 0.38 0.32 0.36 0.39\n",
      "\n",
      "\n",
      "Iteration 30:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.32 0.29 0.32 0.42 0.30\n",
      "U D L D R      | 0.38 0.38 0.34 0.38 0.32\n",
      "L D U R D      | 0.33 0.37 0.31 0.34 0.44\n",
      "U R R R L      | 0.28 0.41 0.39 0.43 0.39\n",
      "\n",
      "\n",
      "Iteration 40:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.32 0.29 0.32 0.43 0.31\n",
      "U D L D R      | 0.38 0.38 0.34 0.40 0.31\n",
      "L D U R D      | 0.33 0.37 0.31 0.34 0.51\n",
      "U R R R L      | 0.28 0.43 0.43 0.48 0.39\n",
      "\n",
      "\n",
      "Iteration 50:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.32 0.29 0.31 0.44 0.32\n",
      "U D L D R      | 0.38 0.39 0.33 0.44 0.31\n",
      "L D U D D      | 0.33 0.39 0.29 0.39 0.54\n",
      "U R R R L      | 0.28 0.46 0.49 0.57 0.39\n",
      "\n",
      "\n",
      "Iteration 60:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.32 0.29 0.31 0.45 0.32\n",
      "U D L D R      | 0.37 0.40 0.31 0.46 0.30\n",
      "L D U D D      | 0.33 0.40 0.29 0.42 0.57\n",
      "U R R R L      | 0.28 0.49 0.51 0.61 0.39\n",
      "\n",
      "\n",
      "Iteration 70:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L D U D D      | 0.32 0.30 0.31 0.46 0.33\n",
      "U D R D D      | 0.37 0.41 0.31 0.48 0.29\n",
      "L D U D D      | 0.33 0.41 0.28 0.43 0.59\n",
      "U R R R L      | 0.27 0.52 0.54 0.63 0.39\n",
      "\n",
      "\n",
      "Iteration 80:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L R U D D      | 0.31 0.30 0.31 0.46 0.33\n",
      "U D R D D      | 0.37 0.42 0.33 0.51 0.30\n",
      "L D D D D      | 0.33 0.43 0.28 0.46 0.61\n",
      "U R R R L      | 0.27 0.55 0.58 0.69 0.39\n",
      "\n",
      "\n",
      "Iteration 90:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "L D U D D      | 0.31 0.30 0.31 0.47 0.33\n",
      "U D R D D      | 0.36 0.42 0.35 0.53 0.31\n",
      "L D D D D      | 0.33 0.44 0.29 0.49 0.62\n",
      "U R R R L      | 0.27 0.57 0.61 0.71 0.39\n",
      "\n",
      "\n",
      "Iteration 100:\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "R D U D D      | 0.32 0.31 0.30 0.48 0.34\n",
      "U D R D D      | 0.36 0.43 0.36 0.55 0.33\n",
      "L D D D D      | 0.32 0.45 0.31 0.52 0.64\n",
      "U R R R L      | 0.27 0.59 0.62 0.74 0.39\n",
      "\n",
      "\n",
      "Final policy parameters (theta):\n",
      "Policy (Direction) | Probabilities (Max)\n",
      "R D U D D      | 0.32 0.31 0.30 0.48 0.34\n",
      "U D R D D      | 0.36 0.43 0.36 0.55 0.33\n",
      "L D D D D      | 0.32 0.45 0.31 0.52 0.64\n",
      "U R R R L      | 0.27 0.59 0.62 0.74 0.39\n",
      "Path taken: [(0, 0), (0, 1), (0, 0), (0, 0), (0, 0), (0, 1), (1, 1), (2, 1), (1, 1), (1, 0), (0, 0), (0, 0), (1, 0), (2, 0), (1, 0), (1, 0), (0, 0), (0, 0), (0, 1), (0, 1), (0, 2), (0, 2), (1, 2), (1, 3), (1, 4), (2, 4), (2, 4), (3, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Actor-Critic algorithm implementation\n",
    "def actor_critic(grid_iterations=100):\n",
    "    for iteration in range(grid_iterations):\n",
    "        state = (0, 0)  # Start state\n",
    "\n",
    "        while state != (3, 4):  # Run until we reach the goal (bottom-right corner)\n",
    "            action = choose_action(state)\n",
    "            next_state, reward = take_step(state, action)\n",
    "            \n",
    "            # Update Actor and Critic based on the step\n",
    "            actor_critic_update(state, action, reward, next_state)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "        # Every 10 iterations, print the policy and probabilities\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            print(f\"Iteration {iteration + 1}:\")\n",
    "            display_policy()\n",
    "            print(\"\\n\")\n",
    "\n",
    "    # Final grid and path taken after training\n",
    "    print(\"Final policy parameters (theta):\")\n",
    "    display_policy()\n",
    "\n",
    "    # Show final path from start to goal\n",
    "    state = (0, 0)\n",
    "    path_taken = [state]\n",
    "    while state != (3, 4):\n",
    "        action = choose_action(state)\n",
    "        state, _ = take_step(state, action)\n",
    "        path_taken.append(state)\n",
    "\n",
    "    print(\"Path taken:\", path_taken)\n",
    "\n",
    "# Call the function to run the Actor-Critic algorithm\n",
    "actor_critic(100)  # Run for 100 iterations\n",
    "actor_critic(100)  # Run for 100 iterations\n",
    "actor_critic(100)  # Run for 100 iterations\n",
    "actor_critic(100)  # Run for 100 iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
